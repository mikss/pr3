{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup\n",
    "\n",
    "Load example data and prepare feature normalization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "import logging\n",
    "from typing import Dict, Tuple\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from sklearn.datasets import load_boston\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "from pr3.pursuit import PracticalProjectionPursuitRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set_style('whitegrid')\n",
    "%matplotlib inline\n",
    "\n",
    "RANDOM_SEED = 2021\n",
    "TRG_RATIO = 0.75\n",
    "np.random.seed(RANDOM_SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "boston = load_boston()\n",
    "print(boston.DESCR)\n",
    "xcols = boston.feature_names\n",
    "ycol = \"MEDV\"\n",
    "\n",
    "df = pd.DataFrame(\n",
    "    data=boston.data,\n",
    "    columns=xcols,\n",
    ")\n",
    "df[ycol] = boston.target\n",
    "trg_idxs = np.random.binomial(1, p=TRG_RATIO, size=df.shape[0]).astype(bool)\n",
    "trg_df = df.iloc[trg_idxs, :].copy()\n",
    "tst_df = df.iloc[~trg_idxs, :].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "\n",
    "@dataclass\n",
    "class FeatureNormalizer:\n",
    "    logarithm: bool = False\n",
    "    winsorize: bool = False\n",
    "    zscore: bool = False\n",
    "    \n",
    "    _logarithm_cols: Dict[int, float] = None\n",
    "    _winsorize_extremes: Dict[int, Tuple[float, float]] = None\n",
    "    _zscore_stats: Dict[int, Tuple[float, float]] = None\n",
    "    \n",
    "    HEAVY_TAILED_SKEW: float = 2.0\n",
    "    CONTINUOUS_UNIQUE_COUNT: int = 5\n",
    "    LOG_SUMMAND_QUANTILE: float = 0.005\n",
    "    EXTREME_QUANTILE: float = 0.005\n",
    "    \n",
    "    def fit(self, x: np.ndarray) -> FeatureNormalizer:\n",
    "        x = x.copy()\n",
    "        if self.logarithm:\n",
    "            x = self._logarithm_fit(x)._logarithm_transform(x)\n",
    "        if self.winsorize:\n",
    "            x = self._winsorize_fit(x)._winsorize_transform(x)\n",
    "        if self.zscore:\n",
    "            x = self._zscore_fit(x)._zscore_transform(x)\n",
    "            \n",
    "        return self\n",
    "        \n",
    "    def transform(self, x: np.ndarray) -> np.ndarray:\n",
    "        x = x.copy()\n",
    "        if self.logarithm:\n",
    "            x = self._logarithm_transform(x)\n",
    "        if self.winsorize:\n",
    "            x = self._winsorize_transform(x)\n",
    "        if self.zscore:\n",
    "            x = self._zscore_transform(x)\n",
    "            \n",
    "        return x\n",
    "        \n",
    "    def _logarithm_fit(self, x: np.ndarray) -> FeatureNormalizer:\n",
    "        skews = ((x - x.mean(axis=0)) ** 3.0).mean(axis=0) / x.var(axis=0) ** 1.5\n",
    "        self._logarithm_cols = {\n",
    "            col: np.quantile(x[x[:, col] > 0, col], self.LOG_SUMMAND_QUANTILE)\n",
    "            for col, skew in enumerate(skews)\n",
    "            if skew > self.HEAVY_TAILED_SKEW\n",
    "            and len(np.unique(x[:, col])) > self.CONTINUOUS_UNIQUE_COUNT\n",
    "            and all(x[:, col] >= 0)\n",
    "        }\n",
    "        return self\n",
    "        \n",
    "    def _winsorize_fit(self, x: np.ndarray) -> FeatureNormalizer:\n",
    "        lows = np.quantile(x, q=self.EXTREME_QUANTILE, axis=0)\n",
    "        highs = np.quantile(x, q=1 - self.EXTREME_QUANTILE, axis=0)\n",
    "        self._winsorize_extremes = dict(zip(range(x.shape[1]), zip(lows, highs)))\n",
    "        return self\n",
    "        \n",
    "    def _zscore_fit(self, x: np.ndarray) -> FeatureNormalizer:\n",
    "        mns = np.mean(x, axis=0)\n",
    "        sds = np.std(x, axis=0)\n",
    "        self._zscore_stats = dict(zip(range(x.shape[1]), zip(mns, sds)))\n",
    "        return self\n",
    "        \n",
    "    def _logarithm_transform(self, x: np.ndarray) -> np.ndarray:\n",
    "        if self._logarithm_cols is None:\n",
    "            raise AttributeError(\"Log transform not yet fit on training data.\")\n",
    "        for col, quantile in self._logarithm_cols.items():\n",
    "            x[:, col] = np.log(quantile + x[:, col])\n",
    "        return x\n",
    "    \n",
    "    def _winsorize_transform(self, x: np.ndarray) -> np.ndarray:\n",
    "        if self._winsorize_extremes is None:\n",
    "            raise AttributeError(\"Winsorization transform not yet fit on training data.\")\n",
    "        for col, extremes in self._winsorize_extremes.items():\n",
    "            x[:, col] = np.clip(x[:, col], extremes[0], extremes[1])\n",
    "        return x\n",
    "    \n",
    "    def _zscore_transform(self, x: np.ndarray) -> np.ndarray:\n",
    "        if self._zscore_stats is None:\n",
    "            raise AttributeError(\"Z-score transform not yet fit on training data.\")\n",
    "        for col, stats in self._zscore_stats.items():\n",
    "            x[:, col] = (x[:, col] - stats[0]) / stats[1]\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model fitting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We fit our projection pursuit regression below, where the key contributor to some loose form of \"interpretability\" is the sparsity constraint introduced by the least angle regression used for projection vector optimization. That is, by limiting the projection vector to have three nonzero coordinates (as specified by the argument `max_iter=3`), it becomes easier to to understand the meaning of each one-dimensional projection, and therefore also to understand the contribution of each ridge function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = FeatureNormalizer(logarithm=True, winsorize=False, zscore=True)\n",
    "trg_x = f.fit(trg_df[xcols].values).transform(trg_df[xcols].values)\n",
    "trg_y = trg_df[ycol].values\n",
    "\n",
    "ppr = PracticalProjectionPursuitRegressor(\n",
    "    n_stages=5,\n",
    "    learning_rate=1.0,\n",
    "    ridge_function_class=\"polynomial\",\n",
    "    ridge_function_kwargs=dict(degree=3),\n",
    "    projection_optimizer_class=\"least_angle\",\n",
    "    projection_optimizer_kwargs=dict(max_iter=3),\n",
    "    random_state=RANDOM_SEED,\n",
    ").fit(trg_x, trg_y)\n",
    "ppr.plot_losses()\n",
    "tst_df['yhat'] = ppr.predict(f.transform(tst_df[xcols].values))\n",
    "print(f\"Test R2: {r2_score(tst_df[ycol], tst_df['yhat']):0.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below, we visualize the learned ridge functions (the nonlinear regression estimates in the one-dimensional projected space). Note that each stage fits against the residuals from previous stages, hence the learned functions do not appear to be good fits to projected data (except in the first stage). Furthermore, any apparent \"gap\" in the fit represents the component of variance explained by earlier stages of training.\n",
    "\n",
    "It can be very tempting to develop _post hoc_ \"just so stories\" upon viewing these plots; it may be safer to register any hypotheses about interpretation ahead of generating the plots below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ppr.plot(\n",
    "    trg_x, \n",
    "    trg_y, \n",
    "    feature_names=xcols, \n",
    "    fig_height=2.5, \n",
    "    fig_width=5.0, \n",
    "    scatter_sample_ratio=0.5,\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
